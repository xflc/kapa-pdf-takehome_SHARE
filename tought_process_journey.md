# Thought Process: First Approach

I started by researching what is currently used. A few options I thought were promising for our setup were: docling, mineru, unstructured.io, and marker. I'll comment on those a bit later. First, some context.

(Note: The first thing I tried was using the OCR feature in PyMuPDF, since extracting text from the scanned PDF was empty. Still, the results were not good, so I moved on from PyMuPDF.)

A few years ago, I did a project to sync audiobooks with autogenerated subtitles, mainly for philosophy books. Many of these academic texts are only available in PDF, so I'm familiar with many limitations of traditional models (for example, I had to train a model to recognize headers and footers to remove them). This experience made me consider at least a hybrid approach that combines LLMs with traditional methods.

To quickly test how well standard LLMs would perform, I simply swapped out the PyMuPDF converter for a basic call to the same model used for chat: gpt-4.1-mini. I know it is not the best model for this use case, but it's rarely the case that we end up going for the best and most expensive model, so I wanted to see how much we can improve just with this simple approach.

The initial results were promising: the model failed to answer only three questions, typically stating that the information was not available. Looking at the generated chunks, I noticed the model often skips extracting all the details from large tables, instead replacing most of the content with phrases like "| (and many more, see pages 3-5...) ".

However, the model does generate chunks for every page of the PDF, suggesting that OpenAI is already splitting the document by page and having the model summarize each one. This makes me somewhat doubtful that simply using a larger or better model will help, since these models are also known for being concise to save on token usage.

Next steps: (1) try a few better models, (2) convert each page to an image to rely more on VLMs, and (3) experiment with more complex pipelines to segment the PDF into sections to then use specialized models + traditional OCR for tables, formulas, etc. (the libraries I mentioned in the beginning of this document implement some of these pipelines, so I might some before implementing the pipelines myself).
